---
title: 'Games Of Thrones'
author: "Laura Mathilde Yong Li Nielsen" 
date: 'created on 22 November 2020 and updated `r format(Sys.time(), "%d %B, %Y")`'
output: html_document
---

# Load the packages and got.pdf
```{r}
knitr::opts_chunk$set(echo = TRUE,
                      warning = FALSE,
                      message = FALSE)
library(tidyverse)
library(here)

# For text mining:
library(pdftools)
library(tidytext)
library(textdata) 
library(ggwordcloud)
```

```{r}
got_path <- here("data", "got.pdf")
got_text <- pdf_text(got_path)
```


## Looking at the PDF and splitting lines
```{r single-page}
got_p9 <- got_text[9]
got_p9
```

```{r split-lines}
got_df <- data.frame(got_text) %>% 
  mutate(text_full = str_split(got_text, pattern = '\\n')) %>% 
  unnest(text_full) %>% 
  mutate(text_full = str_trim(text_full)) 
```

```{r tokensize}
got_tokens <- got_df %>% 
  unnest_tokens(word, text_full)
```

### Counting the words
```{r count-words}
got_wc <- got_tokens %>% 
  count(word) %>% 
  arrange(-n)
got_wc
```
#### Remove stopwords
```{r stopwords}
got_stop <- got_tokens %>% 
  anti_join(stop_words) %>% 
  select(-got_text)
```

Counting without stopwords
```{r count-words2}
got_swc <- got_stop %>% 
  count(word) %>% 
  arrange(-n)
```

Getting rid of numbers
```{r skip-numbers}
got_no_numeric <- got_stop %>% 
  filter(is.na(as.numeric(word)))
```

### A word cloud of GOT report words (non-numeric)

```{r wordcloud-prep}

length(unique(got_no_numeric$word))

got_top100 <- got_no_numeric %>% 
  count(word) %>% 
  arrange(-n) %>% 
  head(100)
```

```{r wordcloud}
got_cloud <- ggplot(data = got_top100, aes(label = word)) +
  geom_text_wordcloud() +
  theme_minimal()

got_cloud
```

Custumizing the wordcloud
```{r wordcloud-pro}
ggplot(data = got_top100, aes(label = word, size = n)) +
  geom_text_wordcloud_area(aes(color = n), shape = "diamond") +
  scale_size_area(max_size = 12) +
  scale_color_gradientn(colors = c("darkgreen","blue","red")) +
  theme_minimal()
```

### Sentiment analysis
```{r afinn}
get_sentiments(lexicon = "afinn")

# Positive words
afinn_pos <- get_sentiments("afinn") %>% 
  filter(value %in% c(3,4,5))

afinn_pos
```

bing: binary, "positive" or "negative"
```{r bing}
get_sentiments(lexicon = "bing")
```

Now nrc:
```{r nrc}
get_sentiments(lexicon = "nrc")
```

Let's do sentiment analysis on the GOT text data using afinn, and nrc. 

### Sentiment analysis with afinn: 

First, bind words in `got_stop` to `afinn` lexicon:
```{r bind-afinn}
got_afinn <- got_stop %>% 
  inner_join(get_sentiments("afinn"))
```

Counts by sentiment ranking and ggplot
```{r count-afinn}
got_afinn_hist <- got_afinn %>% 
  count(value)

ggplot(data = got_afinn_hist, aes(x = value, y = n)) +
  geom_col()
```

Investigate some of the words in a bit more depth:
```{r afinn-2}

got_afinn2 <- got_afinn %>% 
  filter(value == 2)
```

```{r afinn-2-more}

# Check the unique 2-score words:
unique(got_afinn2$word)

# Count & plot them
got_afinn2_n <- got_afinn2 %>% 
  count(word, sort = TRUE) %>% 
  head(10) %>% 
  mutate(word = fct_reorder(factor(word), n))


ggplot(data = got_afinn2_n, aes(x = word, y = n)) +
  geom_col() +
  coord_flip()

```

Summarizing sentiment afinn
```{r summarize-afinn}
got_summary <- got_afinn %>% 
  summarize(
    mean_score = mean(value),
    median_score = median(value)
  )
```

### NRC lexicon for sentiment analysis
```{r bind-bing}
got_nrc <- got_stop %>% 
  inner_join(get_sentiments("nrc"))
```

```{r check-exclusions}
got_exclude <- got_stop %>% 
  anti_join(get_sentiments("nrc"))

# View(got_exclude)

# Count to find the most excluded:
got_exclude_n <- got_exclude %>% 
  count(word, sort = TRUE)

head(got_exclude_n)
```

Now find some counts: 
```{r count-bing}
got_nrc_n <- got_nrc %>% 
  count(sentiment, sort = TRUE)

ggplot(data = got_nrc_n, aes(x = sentiment, y = n)) +
  geom_col()
```


Or count by sentiment *and* word, then facet:
```{r count-nrc}
got_nrc_n5 <- got_nrc %>% 
  count(word,sentiment, sort = TRUE) %>% 
  group_by(sentiment) %>% 
  top_n(5) %>% 
  ungroup()

got_nrc_gg <- ggplot(data = got_nrc_n5, aes(x = reorder(word,n), y = n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, ncol = 2, scales = "free") +
  coord_flip() +
  theme_minimal() +
  labs(x = "Word", y = "count")

# Show it
got_nrc_gg

# Save it
ggsave(plot = got_nrc_gg, 
       here("figures","got_nrc_sentiment.png"), 
       height = 8, 
       width = 5)

```


```{r nrc-confidence}
conf <- get_sentiments(lexicon = "nrc") %>% 
  filter(word == "confidence")

# Yep, check it out:
conf
```

## Questions 
### What are the most common meaningful words and what emotions do you expect will dominate this volume? 

Words such as "honor", "smile", and "sweet" are very common words in the volume and I expect fear, anxiety, loyalty, love, and hate to be dominant emotions in the volume based on the topic and the historical context. 

### Are there any terms that are similarly ambiguous to the 'confidence' above? 

Yes! By using the prior code chunk and change confidence to "birth" a similar ambiguity is evident. "Birth" associates with positive and negative words. 

```{r birth}
birth <- get_sentiments(lexicon = "nrc") %>% 
  filter(word == "birth")

birth
```

### Credits: 
This tutorial is inspired by Allison Horst's Advanced Statistics and Data Analysis.
